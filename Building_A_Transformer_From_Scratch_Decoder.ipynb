{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Randoot/NLP-2/blob/main/Building_A_Transformer_From_Scratch_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7770c8d",
      "metadata": {
        "id": "f7770c8d"
      },
      "source": [
        "### 1. Importing TensorFlow and Other Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d3e6c913",
      "metadata": {
        "id": "d3e6c913"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99dcd4fc",
      "metadata": {
        "id": "99dcd4fc"
      },
      "source": [
        "### 2. Defining Positional Encoding Class\n",
        "![Angel Rates.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ddf78b4",
      "metadata": {
        "id": "4ddf78b4"
      },
      "outputs": [],
      "source": [
        "# Incorporate information about the positions of tokens in a sequence\n",
        "class PositionalEncoding(layers.Layer):\n",
        "  # d_model: The dimensionality of the model = the size of the token embeddings.\n",
        "  #  max_len: The maximum length of the sequences for which positional encoding will be computed.\n",
        "\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        #self.pos_encoding=  This is the positional encoding matrix\n",
        "        # calculated using the positional_encoding method.\n",
        "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
        "#calculate the angles used in the positional encoding formula\n",
        "#pos: A vector of position indices.\n",
        "#i: A vector of the dimension indices.\n",
        "#d_model: The dimensionality of the embeddings\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        #scales the position indices by these rates\n",
        "        # giving different frequencies for different dimensions.\n",
        "        return pos * angle_rates\n",
        "\n",
        "#generates the actual positional encoding tensor.\n",
        "    def positional_encoding(self, position, d_model):\n",
        "\n",
        "#angle_rads is calculated by applying get_angles over the entire range\n",
        "# of positions and dimensions.\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
        "                                     np.arange(d_model)[np.newaxis, :], d_model)\n",
        "        # apply sine and cosine functions to alternate dimensions\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "#defines the forward pass of the layer\n",
        "# inputs = tensor representing token embeddings\n",
        "    def call(self, inputs):\n",
        "      #positional encodings are added to the token embeddings\n",
        "      # slicing ensures that the positional encodings match the length of the input sequence.\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7903aef5",
      "metadata": {
        "id": "7903aef5"
      },
      "source": [
        "- **Explanation**: This class implements positional encoding, which is essential in Transformer models to provide a sense of order to the sequence of inputs. It calculates and applies sine and cosine transformations to create the encoding for each position in the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf84334",
      "metadata": {
        "id": "ccf84334"
      },
      "source": [
        "### 3. Scaled Dot-Product Attention\n",
        "![scaling by dim.png](data:image/png;base64,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)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cca92485",
      "metadata": {
        "id": "cca92485"
      },
      "outputs": [],
      "source": [
        "#computes the attention scores and output for a given set of queries, keys, and values,\n",
        "# incorporating optional masking\n",
        "#q: The query matrix.\n",
        "#k: The key matrix.\n",
        "#v: The value matrix.\n",
        "#mask: An optional tensor used to mask out certain positions (future tokens).\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  #the dot products between queries and keys.\n",
        "  # The transpose_b=True argument transposes the keys matrix\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "#Scale the Attention Scores\n",
        "#dk is the dimensionality of the keys or queries).\n",
        "#It is cast to a floating-point number.\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    # check the equation\n",
        "    #raw attention scores are scaled by dividing by the square root of dk.\n",
        "    #This scaling helps to stabilize gradients during training\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "      #If a mask is provided, it is used to modify the attention logits.\n",
        "     #The mask is added to the scaled logits where masked positions are multiplied by -1e9\n",
        "    #(a very large negative number).\n",
        "     #This effectively sets those positions to zero after applying the softmax function,\n",
        "     #making sure that the corresponding attention weights become zero.\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "#Compute the Attention Weights\n",
        "# converts the scaled attention logits into probabilities.\n",
        "#The axis=-1 argument ensures that the softmax is applied across the last dimension,\n",
        "#which corresponds to the keys.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "#computes the weighted sum of the values,\n",
        "#where the weights are the attention weights obtained from the softmax operation.\n",
        "#This produces the final output tensor,\n",
        "# which combines information from the value matrix weighted by how relevant each key is to the query.\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "#output= The result of applying the attention weights to the values.\n",
        "#attention_weights= The attention weights matrix, which indicates how much focus each query has on each key.\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178da757",
      "metadata": {
        "id": "178da757"
      },
      "source": [
        "- **Explanation**: This function computes scaled dot-product attention, which forms the core of attention mechanisms in transformers. It takes the query, key, and value matrices, scales the dot product of the query and key, applies a mask (if provided), and finally computes the attention-weighted output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c788a0b8",
      "metadata": {
        "id": "c788a0b8"
      },
      "source": [
        "### 4. Multi-Head Attention (Truncated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1035be96",
      "metadata": {
        "id": "1035be96"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6839689e",
      "metadata": {
        "id": "6839689e"
      },
      "source": [
        "- **Explanation**: The implementation for the `MultiHeadAttention` class is truncated, but it follows the transformer architecture where multiple attention heads are used to attend to different parts of the input sequence simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7a8c42",
      "metadata": {
        "id": "bd7a8c42"
      },
      "source": [
        "### 5. Point-Wise Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "113ef955",
      "metadata": {
        "id": "113ef955"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return keras.Sequential([\n",
        "        layers.Dense(dff, activation='relu'),\n",
        "        layers.Dense(d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531bcaae",
      "metadata": {
        "id": "531bcaae"
      },
      "source": [
        "Explanation:\n",
        "* This function creates a point-wise feed forward network, which is a key component in Transformer models.\n",
        "* It uses two fully connected dense layers.\n",
        "* The first layer has a hidden dimension of `dff` and applies the `ReLU` activation function to introduce non-linearity.\n",
        "* The second layer projects the output back to the original dimensionality `d_model`, without any activation, making it a linear transformation.\n",
        "* This kind of network is applied independently to each position in the sequence, which makes it \"point-wise.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a38a6dd",
      "metadata": {
        "id": "5a38a6dd"
      },
      "source": [
        "### 6. Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "84013495",
      "metadata": {
        "id": "84013495"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attn1 = self.mha1(x, x, x, mask)\n",
        "        attn1 = self.dropout1(attn1, training=True)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=True)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc59329",
      "metadata": {
        "id": "fdc59329"
      },
      "source": [
        "- **Explanation**:\n",
        "    - **Initialization (`__init__`)**:\n",
        "        - The `DecoderLayer` is a core building block of the Transformer decoder. It consists of two main components:\n",
        "            1. Multi-Head Attention (`mha1`): This layer performs self-attention on the input sequence `x`, allowing the decoder to weigh the importance of different tokens in the sequence.\n",
        "            2. Feed-Forward Network (`ffn`): After attention is applied, the data passes through a point-wise feed-forward network that consists of two dense layers.\n",
        "        - Layer Normalization is applied twice—before and after the feed-forward network—helping stabilize the network's training.\n",
        "        - Dropout is used to prevent overfitting by randomly deactivating some neurons during training.\n",
        "\n",
        "    - **Forward Pass (`call`)**:\n",
        "        - **Step 1**: Multi-head attention is applied to the input `x`, and the result is stored in `attn1`. Dropout is applied to regularize the attention output.\n",
        "        - **Step 2**: The result from the attention layer (`attn1`) is added back to the input `x` (residual connection) and normalized using `layernorm1`. This produces `out1`.\n",
        "        - **Step 3**: The `out1` result is passed through the feed-forward network (`ffn_output`), and dropout is applied again for regularization.\n",
        "        - **Step 4**: The output of the feed-forward network is added back to `out1` (residual connection) and normalized again via `layernorm2`, producing the final output (`out2`).\n",
        "    \n",
        "    - **Residual Connections**: The addition of the input (`x`) to the output of the attention and feed-forward networks is a crucial part of the Transformer architecture. It ensures that the model retains information from earlier layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbadb435",
      "metadata": {
        "id": "dbadb435"
      },
      "source": [
        "### 7. Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3bbf6629",
      "metadata": {
        "id": "3bbf6629"
      },
      "outputs": [],
      "source": [
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_positional_encoding)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        x = self.dropout(x, training=True)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.dec_layers[i](x, mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda07be5",
      "metadata": {
        "id": "dda07be5"
      },
      "source": [
        "- **Explanation**:\n",
        "    - **Initialization (`__init__`)**:\n",
        "        - The `Decoder` class represents the entire decoder module in the Transformer model, consisting of multiple `DecoderLayer` instances stacked together.\n",
        "        - **Attributes**:\n",
        "            - `d_model`: The dimensionality of the embedding space.\n",
        "            - `num_layers`: The number of stacked decoder layers.\n",
        "            - **Embedding Layer**: The target sequence is first passed through an embedding layer that converts tokens to dense vectors of dimension `d_model`.\n",
        "            - **Positional Encoding**: Since the Transformer model does not have recurrence, positional encoding is applied to provide information about the order of the sequence.\n",
        "            - **Decoder Layers**: Multiple `DecoderLayer` instances (as defined earlier) are created and stored in the list `dec_layers`.\n",
        "            - **Dropout**: Dropout is applied after the embedding to regularize the decoder and prevent overfitting.\n",
        "\n",
        "    - **Forward Pass (`call`)**:\n",
        "        - **Input Processing**:\n",
        "            - The input sequence `x` is passed through the embedding layer, which converts tokens into dense vectors.\n",
        "            - The embeddings are then scaled by the square root of the model's dimension (`d_model`) to normalize the variance in the output.\n",
        "            - Positional encoding is applied to give each position in the sequence a unique representation.\n",
        "            - Dropout is applied to the positional encoding output.\n",
        "        \n",
        "        - **Decoder Layers**: The input sequence is passed sequentially through each of the decoder layers, applying self-attention and feed-forward transformations in each layer.\n",
        "\n",
        "    - **Final Output**:\n",
        "        - The final output of the `Decoder` is a tensor with the shape `(batch_size, target_seq_len, d_model)`, where each token in the target sequence is now represented in the `d_model`-dimensional space, ready to be used for further processing (such as generating predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc774b08",
      "metadata": {
        "id": "bc774b08"
      },
      "source": [
        "### 8. Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "966cebcd",
      "metadata": {
        "id": "966cebcd"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(seq_len):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "class TransformerDecoderModel(keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding, rate=0.1):\n",
        "        super(TransformerDecoderModel, self).__init__()\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding, rate)\n",
        "        self.final_layer = layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, tar):\n",
        "        seq_len = tf.shape(tar)[1]\n",
        "        look_ahead_mask = create_look_ahead_mask(seq_len)\n",
        "\n",
        "        dec_output = self.decoder(tar, look_ahead_mask)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f81f6c5",
      "metadata": {
        "id": "1f81f6c5"
      },
      "source": [
        "- **Explanation**:\n",
        "    - **Look-Ahead Mask (`create_look_ahead_mask`)**:\n",
        "        - The function `create_look_ahead_mask` creates a mask to prevent the decoder from attending to future tokens in the sequence during training.\n",
        "        - The mask is created using TensorFlow's `band_part` function, which generates a lower triangular matrix filled with ones below the diagonal and zeros elsewhere. The diagonal and lower portion are kept, while the upper portion is masked.\n",
        "        - This is essential for autoregressive tasks like language generation, where the model should not see future tokens.\n",
        "\n",
        "    - **TransformerDecoderModel**:\n",
        "        - **Initialization (`__init__`)**:\n",
        "            - This class defines a Transformer decoder model, where the decoder is followed by a final dense layer for generating predictions.\n",
        "            - **Attributes**:\n",
        "                - `decoder`: This is an instance of the `Decoder` class, which consists of multiple layers of attention and feed-forward operations.\n",
        "                - `final_layer`: A dense layer that maps the output of the decoder to the target vocabulary size. This layer is responsible for producing the logits used for token predictions.\n",
        "\n",
        "    - **Forward Pass (`call`)**:\n",
        "        - **Input Processing**:\n",
        "            - The `call` method takes in the target sequence `tar` and computes the sequence length.\n",
        "            - The look-ahead mask is created using the `create_look_ahead_mask` function, which ensures that during training, the model cannot look ahead to future positions in the target sequence.\n",
        "        \n",
        "        - **Decoder**:\n",
        "            - The target sequence, along with the look-ahead mask, is passed to the `Decoder`, which applies the necessary layers (self-attention, feed-forward networks, etc.) to produce the decoder output.\n",
        "        \n",
        "        - **Final Output**:\n",
        "            - The output of the decoder is passed through the `final_layer`, which converts the output into a tensor with dimensions corresponding to the target vocabulary size. This is the prediction layer where the model generates probabilities for each token in the vocabulary.\n",
        "            - The final output is a tensor of shape `(batch_size, target_seq_len, target_vocab_size)` that contains the logits for the vocabulary for each token in the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c30ed9a",
      "metadata": {
        "id": "2c30ed9a"
      },
      "source": [
        "### 9. Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "236174da",
      "metadata": {
        "id": "236174da"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "target_vocab_size = 8000\n",
        "max_positional_encoding = 10000\n",
        "\n",
        "decoder_model = TransformerDecoderModel(num_layers, d_model, num_heads, dff, target_vocab_size, max_positional_encoding)\n",
        "\n",
        "decoder_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "929c5db5",
      "metadata": {
        "id": "929c5db5"
      },
      "outputs": [],
      "source": [
        "text_file_path = 'shakespeare.txt'\n",
        "with open(text_file_path, 'r', encoding='utf-8') as file:\n",
        "    text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bfef5e3f",
      "metadata": {
        "id": "bfef5e3f"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "sequence_length = 50\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "for i in range(0, len(sequences) - sequence_length):\n",
        "    input_sequences.append(sequences[i:i + sequence_length])\n",
        "    target_sequences.append(sequences[i + 1:i + sequence_length + 1])\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "target_sequences = np.array(target_sequences)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "82d6c901",
      "metadata": {
        "id": "82d6c901"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ab6c7e01",
      "metadata": {
        "id": "ab6c7e01"
      },
      "outputs": [],
      "source": [
        "target_vocab_size = vocab_size\n",
        "\n",
        "decoder_model = TransformerDecoderModel(\n",
        "    num_layers=4, d_model=128, num_heads=8, dff=512,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    max_positional_encoding=sequence_length\n",
        ")\n",
        "\n",
        "decoder_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c1de993c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1de993c",
        "outputId": "7b1f0622-0e47-4c86-c43c-97ecbe5ef33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1472/1472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1194s\u001b[0m 799ms/step - loss: 2.1332\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7da68e473160>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "epochs = 1\n",
        "decoder_model.fit(dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab260150",
      "metadata": {
        "id": "ab260150"
      },
      "source": [
        "### 10. Generating Text with the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4d3ad295",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d3ad295",
        "outputId": "b17497ab-f10b-4c95-856e-b8ef5c7df207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be, that is he in true my gotle,\n",
            "but love is shere priting of true,\n",
            "buth with holy new,  maystand it which look\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, tokenizer, seed_text, num_generate=100):\n",
        "    input_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    input_sequence = np.array(input_sequence).reshape(1, -1)\n",
        "\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_sequence)\n",
        "        predictions = predictions[:, -1, :]  # Get the last predicted token\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        predicted_char = tokenizer.index_word[predicted_id]\n",
        "        generated_text += predicted_char\n",
        "\n",
        "        input_sequence = np.append(input_sequence[0], predicted_id)[-sequence_length:].reshape(1, -1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"To be, or not to be, that is\"\n",
        "generated = generate_text(decoder_model, tokenizer, seed_text)\n",
        "print(generated)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}