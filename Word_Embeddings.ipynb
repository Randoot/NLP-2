{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Randoot/NLP-2/blob/main/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Word Embeddings\n",
        "\n",
        "Word embeddings are a type of word representation that allows words to be represented as continuous vectors in a high-dimensional space.\n",
        "\n",
        "It aims to map words to vectors of real numbers in such a way that words with similar meanings have similar vector representations.\n",
        "Unlike traditional representations like Bag of Words (BoW), word embeddings capture semantic meanings and relationships between words by placing similar words closer together in the vector space.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Word Embedding**: A dense vector representation of a word where each dimension captures some aspect of its meaning.\n",
        "2. **Pre-trained Embeddings**: Embeddings learned from large corpora, such as Word2Vec, GloVe, and FastText.\n",
        "3. **Semantic Similarity**: Words with similar meanings will have similar embeddings, making it easier to perform tasks like word similarity and analogy."
      ],
      "metadata": {
        "id": "ihBopOobLb2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n"
      ],
      "metadata": {
        "id": "J14EKzDQLg2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load pre-trained Word2Vec model (Google News vectors)\n",
        "Note: This model is quite large. For demonstration, use a smaller or different model as needed.\n",
        " model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "6YBv0QyqLiSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, we'll use a smaller pre-trained model available in gensim\n",
        "from gensim.downloader import load\n",
        "model = load('glove-wiki-gigaword-50')\n",
        "# GloVe (Global Vectors for Word Representation) model trained on the Wikipedia and Gigaword corpus with 50-dimensional vectors.\n",
        "# So each word is represented by a 50-dimensional vector."
      ],
      "metadata": {
        "id": "Iyp8HlpbLmHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad0c381-718d-400d-f020-d3fbc25d755d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example words\n",
        "words = ['king', 'queen', 'man', 'woman']"
      ],
      "metadata": {
        "id": "xkigRBgHLoiP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings\n",
        "embeddings = {word: model[word] for word in words}"
      ],
      "metadata": {
        "id": "xJngiE2qLqOr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each value is a float representing the position of the word in that dimension of the embedding space.\n",
        "\n",
        "The values are typically in a real number range, which can be positive, negative, or zero. The exact range depends on the embedding model and its initialization."
      ],
      "metadata": {
        "id": "xYvYW93DLOtY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H01quYFFLWE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c507469-b3bd-4487-f5a0-94099b2eb7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: king\n",
            "Embedding: [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
            "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
            "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
            " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
            " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
            "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
            " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
            " -0.51042 ]\n",
            "\n",
            "Word: queen\n",
            "Embedding: [ 0.37854    1.8233    -1.2648    -0.1043     0.35829    0.60029\n",
            " -0.17538    0.83767   -0.056798  -0.75795    0.22681    0.98587\n",
            "  0.60587   -0.31419    0.28877    0.56013   -0.77456    0.071421\n",
            " -0.5741     0.21342    0.57674    0.3868    -0.12574    0.28012\n",
            "  0.28135   -1.8053    -1.0421    -0.19255   -0.55375   -0.054526\n",
            "  1.5574     0.39296   -0.2475     0.34251    0.45365    0.16237\n",
            "  0.52464   -0.070272  -0.83744   -1.0326     0.45946    0.25302\n",
            " -0.17837   -0.73398   -0.20025    0.2347    -0.56095   -2.2839\n",
            "  0.0092753 -0.60284  ]\n",
            "\n",
            "Word: man\n",
            "Embedding: [-0.094386  0.43007  -0.17224  -0.45529   1.6447    0.40335  -0.37263\n",
            "  0.25071  -0.10588   0.10778  -0.10848   0.15181  -0.65396   0.55054\n",
            "  0.59591  -0.46278   0.11847   0.64448  -0.70948   0.23947  -0.82905\n",
            "  1.272     0.033021  0.2935    0.3911   -2.8094   -0.70745   0.4106\n",
            "  0.3894   -0.2913    2.6124   -0.34576  -0.16832   0.25154   0.31216\n",
            "  0.31639   0.12539  -0.012646  0.22297  -0.56585  -0.086264  0.62549\n",
            " -0.0576    0.29375   0.66005  -0.53115  -0.48233  -0.97925   0.53135\n",
            " -0.11725 ]\n",
            "\n",
            "Word: woman\n",
            "Embedding: [-1.8153e-01  6.4827e-01 -5.8210e-01 -4.9451e-01  1.5415e+00  1.3450e+00\n",
            " -4.3305e-01  5.8059e-01  3.5556e-01 -2.5184e-01  2.0254e-01 -7.1643e-01\n",
            "  3.0610e-01  5.6127e-01  8.3928e-01 -3.8085e-01 -9.0875e-01  4.3326e-01\n",
            " -1.4436e-02  2.3725e-01 -5.3799e-01  1.7773e+00 -6.6433e-02  6.9795e-01\n",
            "  6.9291e-01 -2.6739e+00 -7.6805e-01  3.3929e-01  1.9695e-01 -3.5245e-01\n",
            "  2.2920e+00 -2.7411e-01 -3.0169e-01  8.5286e-04  1.6923e-01  9.1433e-02\n",
            " -2.3610e-02  3.6236e-02  3.4488e-01 -8.3947e-01 -2.5174e-01  4.2123e-01\n",
            "  4.8616e-01  2.2325e-02  5.5760e-01 -8.5223e-01 -2.3073e-01 -1.3138e+00\n",
            "  4.8764e-01 -1.0467e-01]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display embeddings\n",
        "for word, vector in embeddings.items():\n",
        "    print(f\"Word: {word}\\nEmbedding: {vector}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words\n",
        "similar_words = model.most_similar('computer', topn=5)\n",
        "\n",
        "# TODO:: Display similar words"
      ],
      "metadata": {
        "id": "rKCazO0NLsOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve analogy\n",
        "analogy_result = model.most_similar(positive=['queen', 'man'], negative=['king'], topn=1)\n",
        "\n",
        "# TODO:: Display result"
      ],
      "metadata": {
        "id": "fkcpJPKvL43C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}